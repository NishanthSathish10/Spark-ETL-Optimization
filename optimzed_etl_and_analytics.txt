--- Starting Optimized ETL Job (Split-Era Strategy) ---
Reading lookup table...
Reading Batch 1: 2011 to June 2018 (Integer Schema)...
Reading Batch 2: July 2018 to 2024 (Double Schema)...
Unioning datasets...
Calculating trip duration...
Filtering invalid rows...
Performing BROADCAST Joins...
Caching transformed dataset...
Traceback (most recent call last):
  File "/Users/nish/UMass/532 - Systems for Data Science/project/Spark-ETL-Optimization/optimized_etl_and_analytics.py", line 178, in <module>
    run_optimized_etl(spark)
    ~~~~~~~~~~~~~~~~~^^^^^^^
  File "/Users/nish/UMass/532 - Systems for Data Science/project/Spark-ETL-Optimization/optimized_etl_and_analytics.py", line 155, in run_optimized_etl
    print(f"Total processed rows: {df_joined.count()}")
                                   ~~~~~~~~~~~~~~~^^
  File "/Users/nish/UMass/532 - Systems for Data Science/project/Spark-ETL-Optimization/.venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/classic/dataframe.py", line 439, in count
    return int(self._jdf.count())
               ~~~~~~~~~~~~~~~^^
  File "/Users/nish/UMass/532 - Systems for Data Science/project/Spark-ETL-Optimization/.venv/lib/python3.13/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
        answer, self.gateway_client, self.target_id, self.name)
  File "/Users/nish/UMass/532 - Systems for Data Science/project/Spark-ETL-Optimization/.venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
  File "/Users/nish/UMass/532 - Systems for Data Science/project/Spark-ETL-Optimization/.venv/lib/python3.13/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
        "An error occurred while calling {0}{1}{2}.\n".
        format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o211.count.
: org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Encountered error while reading file file:///Users/nish/UMass/532%20-%20Systems%20for%20Data%20Science/project/Spark-ETL-Optimization/data/raw/yellow_tripdata_2018-10.parquet.  SQLSTATE: KD001
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:856)
	at org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)
	at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:121)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:292)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:232)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)
	at org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:336)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
	at org.apache.spark.scheduler.Task.run(Task.scala:147)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file file:/Users/nish/UMass/532 - Systems for Data Science/project/Spark-ETL-Optimization/data/raw/yellow_tripdata_2018-10.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:280)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:292)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)
	... 29 more
Caused by: java.lang.ClassCastException: class org.apache.spark.sql.catalyst.expressions.MutableLong cannot be cast to class org.apache.spark.sql.catalyst.expressions.MutableDouble (org.apache.spark.sql.catalyst.expressions.MutableLong and org.apache.spark.sql.catalyst.expressions.MutableDouble are in unnamed module of loader 'app')
	at org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setDouble(SpecificInternalRow.scala:284)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setDouble(ParquetRowConverter.scala:186)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addDouble(ParquetRowConverter.scala:97)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$2.writeValue(ColumnReaderBase.java:287)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:486)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:425)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:249)
	... 36 more

